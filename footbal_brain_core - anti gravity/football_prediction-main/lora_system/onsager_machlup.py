"""
ğŸŒ€ ONSAGER-MACHLUP YÃ–RÃœNGE Ä°NTEGRALÄ°
=====================================

Fitness sadece "ÅŸu an" deÄŸil, "TÃœM TARÄ°HÃ‡E"!

Onsager-Machlup Fonksiyoneli:

S_OM(Ï†) = âˆ«[0,T] [(dÏ†/dt + âˆ‡U)^2 / (4T) + âˆ‡Â·V] dt

Anlam:
  â€¢ Birinci terim: "Ne kadar zorlandÄ±?" (Newton cost!)
  â€¢ Ä°kinci terim: "Parametre uzayÄ± ne kadar deÄŸiÅŸti?" (Entropi!)

En dÃ¼ÅŸÃ¼k eylem (S_OM) = En iyi yÃ¶rÃ¼nge!
"""

import torch
import math
from typing import List, Dict


class OnsagerMachlup:
    """
    YÃ¶rÃ¼nge integrali hesaplama
    """
    
    def __init__(self, temperature: float = 0.01):
        """
        Args:
            temperature: Sistem sÄ±caklÄ±ÄŸÄ± (T)
        """
        self.T = temperature
        print(f"ğŸŒ€ Onsager-Machlup baÅŸlatÄ±ldÄ± (T={temperature})")
    
    def calculate_action(
        self,
        lora,
        trajectory: List[Dict] = None
    ) -> Dict:
        """
        LoRA'nÄ±n yÃ¶rÃ¼nge eylemini hesapla!
        
        S_OM = âˆ« [(dÎ¸/dt + âˆ‡U)^2 / (4T) + div] dt
        
        Args:
            lora: LoRA instance
            trajectory: YÃ¶rÃ¼nge geÃ§miÅŸi (opsiyonel)
        
        Returns:
            {
                'action': S_OM deÄŸeri,
                'newton_cost': Ä°lk terim (zorluk!),
                'entropy_term': Ä°kinci terim (Ã§eÅŸitlilik!)
            }
        """
        # YÃ¶rÃ¼nge bilgisi yoksa basit hesapla
        if trajectory is None:
            trajectory = self._reconstruct_trajectory(lora)
        
        if len(trajectory) < 2:
            return {'action': 0.0, 'newton_cost': 0.0, 'entropy_term': 0.0}
        
        total_action = 0.0
        total_newton = 0.0
        total_entropy = 0.0
        
        # Her adÄ±m iÃ§in
        for i in range(len(trajectory) - 1):
            theta_t = trajectory[i]['params']
            theta_t1 = trajectory[i+1]['params']
            grad_t = trajectory[i]['gradient']
            
            dt = 1.0  # Zaman adÄ±mÄ±
            
            # 1) PARAMETRE DEÄÄ°ÅÄ°MÄ°: dÎ¸/dt
            dtheta_dt = (theta_t1 - theta_t) / dt
            
            # 2) NEWTON TERÄ°MÄ°: (dÎ¸/dt + âˆ‡U)^2 / (4T)
            # âˆ‡U = gradyan
            deviation = dtheta_dt + grad_t
            newton_cost = torch.sum(deviation ** 2).item() / (4 * self.T)
            
            # 3) ENTROPÄ° TERÄ°MÄ°: âˆ‡Â·V (Diverjans!)
            # Basit yaklaÅŸÄ±m: Parametre deÄŸiÅŸiminin varyansÄ±
            entropy_term = torch.var(theta_t1 - theta_t).item()
            
            # 4) TOPLAM EYLEM
            action_t = (newton_cost + entropy_term) * dt
            
            total_action += action_t
            total_newton += newton_cost * dt
            total_entropy += entropy_term * dt
        
        return {
            'action': total_action,
            'newton_cost': total_newton,
            'entropy_term': total_entropy,
            'trajectory_length': len(trajectory),
            'efficiency': 1.0 / (total_action + 1e-8)  # DÃ¼ÅŸÃ¼k eylem = YÃ¼ksek verimlilik!
        }
    
    def _reconstruct_trajectory(self, lora) -> List[Dict]:
        """
        LoRA'nÄ±n geÃ§miÅŸinden yÃ¶rÃ¼nge rekonstrÃ¼ksiyonu
        
        (GerÃ§ek uygulamada LoRA her adÄ±mda parametrelerini kaydetmeli!)
        """
        trajectory = []
        
        # EÄŸer LoRA'nÄ±n param geÃ§miÅŸi varsa
        if hasattr(lora, 'param_history') and len(lora.param_history) > 0:
            for entry in lora.param_history:
                trajectory.append({
                    'params': entry.get('params'),
                    'gradient': entry.get('gradient', torch.zeros_like(entry['params']))
                })
        else:
            # Yoksa ÅŸu anki parametrelerle dummy yÃ¶rÃ¼nge
            current_params = lora.get_all_lora_params()
            trajectory.append({
                'params': current_params,
                'gradient': torch.zeros_like(current_params)
            })
        
        return trajectory
    
    def compare_loras_by_action(
        self,
        lora_list: List
    ) -> List[tuple]:
        """
        LoRA'larÄ± eylemlerine gÃ¶re karÅŸÄ±laÅŸtÄ±r!
        
        Returns:
            [(lora, action_data), ...] (KÃ¼Ã§Ã¼kten bÃ¼yÃ¼ÄŸe sÄ±ralÄ±!)
        """
        results = []
        
        for lora in lora_list:
            try:
                action_data = self.calculate_action(lora)
                results.append((lora, action_data))
            except:
                continue
        
        # Eyleme gÃ¶re sÄ±rala (KÃ¼Ã§Ã¼kten bÃ¼yÃ¼ÄŸe!)
        # DÃ¼ÅŸÃ¼k eylem = Verimli yÃ¶rÃ¼nge!
        results.sort(key=lambda x: x[1]['action'])
        
        return results


# Global instance
onsager_machlup = OnsagerMachlup(temperature=0.01)



