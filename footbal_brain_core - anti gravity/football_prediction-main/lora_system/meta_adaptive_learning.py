"""
ðŸ§  META-ADAPTIF Ã–ÄžRENME HIZI
==============================

Her LoRA kendi learning rate'ini bulur!

SABÄ°T LEARNING RATE YOK!

NASIL Ã‡ALIÅžIR:
1. BaÅŸlangÄ±Ã§: MizaÃ§ bazlÄ± (SabÄ±rlÄ±: yavaÅŸ, DÃ¼rtÃ¼sel: hÄ±zlÄ±)
2. BaÅŸarÄ±lÄ±ysa â†’ HÄ±zlan!
3. BaÅŸarÄ±sÄ±zsa â†’ YavaÅŸla!
4. Overfit tespiti â†’ DÃ¼ÅŸÃ¼r!
5. Underfit tespiti â†’ ArtÄ±r!

Her LoRA kendi optimal hÄ±zÄ±nÄ± bulur!
"""

from typing import Dict, List, Tuple
import numpy as np


class MetaAdaptiveLearning:
    """
    Meta-adaptif Ã¶ÄŸrenme hÄ±z sistemi
    """
    
    def __init__(self):
        # Her LoRA'nÄ±n Ã¶ÄŸrenme hÄ±z geÃ§miÅŸi
        self.learning_rates = {}  # lora_id -> current_lr
        self.lr_history = {}      # lora_id -> [lr_history]
        self.performance_history = {}  # lora_id -> [(lr, performance)]
    
    def initialize_learning_rate(self, lora, base_lr: float = 0.0001) -> float:
        """
        BaÅŸlangÄ±Ã§ learning rate'i belirle (MÄ°ZAÃ‡ BAZLI!)
        
        Args:
            lora: LoRA instance
            base_lr: Base learning rate
        
        Returns:
            Ä°lk learning rate
        """
        temp = lora.temperament
        
        # MÄ°ZAÃ‡ FAKTÃ–RLERI
        patience = temp.get('patience', 0.5)
        impulsiveness = temp.get('impulsiveness', 0.5)
        risk_appetite = temp.get('risk_appetite', 0.5)
        
        # FORMÃœL:
        # SabÄ±rlÄ± â†’ YavaÅŸ Ã¶ÄŸren (dikkatli!)
        # DÃ¼rtÃ¼sel â†’ HÄ±zlÄ± Ã¶ÄŸren (agresif!)
        # Risk sever â†’ HÄ±zlÄ± Ã¶ÄŸren
        
        temperament_multiplier = (
            (1.0 - patience) * 0.40 +      # SabÄ±rsÄ±z â†’ hÄ±zlÄ±
            impulsiveness * 0.35 +          # DÃ¼rtÃ¼sel â†’ hÄ±zlÄ±
            risk_appetite * 0.25            # Risk sever â†’ hÄ±zlÄ±
        )
        
        # 0.5 - 2.0 arasÄ±
        temperament_multiplier = 0.5 + (temperament_multiplier * 1.5)
        
        initial_lr = base_lr * temperament_multiplier
        
        # Kaydet
        self.learning_rates[lora.id] = initial_lr
        self.lr_history[lora.id] = [initial_lr]
        self.performance_history[lora.id] = []
        
        return initial_lr
    
    def adapt_learning_rate(self, lora, recent_performance: List[float], 
                           current_lr: float = None) -> Tuple[float, str]:
        """
        Learning rate'i adapte et! (META-LEARNING!)
        
        Args:
            lora: LoRA instance
            recent_performance: Son 10 maÃ§Ä±n fitness'Ä±
            current_lr: Mevcut learning rate
        
        Returns:
            (new_lr, reason)
        """
        if current_lr is None:
            current_lr = self.learning_rates.get(lora.id, 0.0001)
        
        if len(recent_performance) < 5:
            return current_lr, "Yetersiz veri"
        
        # ============================================
        # PERFORMANS ANALÄ°ZÄ°
        # ============================================
        
        # Trend (yÃ¼kseliyor mu?)
        first_half = recent_performance[:len(recent_performance)//2]
        second_half = recent_performance[len(recent_performance)//2:]
        
        trend = np.mean(second_half) - np.mean(first_half)
        
        # Variance (stabil mi?)
        variance = np.var(recent_performance)
        
        # Son performans
        recent_avg = np.mean(recent_performance[-5:])
        
        # ============================================
        # KARAR (AKIÅžKAN FORMÃœL!)
        # ============================================
        
        adjustment = 1.0  # Ã‡arpan (1.0 = deÄŸiÅŸmez)
        reason = ""
        
        # SENARYO 1: YÃ¼kseliyor + DÃ¼ÅŸÃ¼k variance â†’ HIZLAN!
        if trend > 0.05 and variance < 0.02:
            adjustment = 1.15  # %15 artÄ±r
            reason = "Performans yÃ¼kseliyor, hÄ±zlanÄ±yorum!"
        
        # SENARYO 2: DÃ¼ÅŸÃ¼yor â†’ YAVAÅžLA!
        elif trend < -0.05:
            adjustment = 0.85  # %15 dÃ¼ÅŸÃ¼r
            reason = "Performans dÃ¼ÅŸÃ¼yor, yavaÅŸlÄ±yorum"
        
        # SENARYO 3: YÃ¼ksek variance â†’ OVERFIT! YavaÅŸla!
        elif variance > 0.05:
            adjustment = 0.80  # %20 dÃ¼ÅŸÃ¼r
            reason = "Ã‡ok dalgalÄ± (overfit?), yavaÅŸlÄ±yorum"
        
        # SENARYO 4: DÃ¼ÅŸÃ¼k performans + DÃ¼ÅŸÃ¼k variance â†’ UNDERFIT! HÄ±zlan!
        elif recent_avg < 0.50 and variance < 0.01:
            adjustment = 1.20  # %20 artÄ±r
            reason = "Underfit, daha agresif Ã¶ÄŸreniyorum!"
        
        # SENARYO 5: Ä°yi performans â†’ KORU!
        elif recent_avg > 0.70:
            adjustment = 1.0  # DeÄŸiÅŸtirme
            reason = "Performans iyi, deÄŸiÅŸtirmiyorum"
        
        # DeÄŸiÅŸtirme
        else:
            adjustment = 1.0
            reason = "Stabil"
        
        # YENÄ° LEARNING RATE
        new_lr = current_lr * adjustment
        
        # SÄ±nÄ±rla (0.00001 - 0.001 arasÄ±)
        new_lr = max(0.00001, min(0.001, new_lr))
        
        # Kaydet
        self.learning_rates[lora.id] = new_lr
        self.lr_history[lora.id].append(new_lr)
        self.performance_history[lora.id].append((new_lr, recent_avg))
        
        return new_lr, reason
    
    def get_optimal_lr_for_lora(self, lora) -> float:
        """
        LoRA'nÄ±n mevcut optimal learning rate'i
        """
        return self.learning_rates.get(lora.id, 0.0001)


# Global instance
meta_adaptive_learning = MetaAdaptiveLearning()

