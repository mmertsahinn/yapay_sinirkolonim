"""
ðŸ”„ LoRA SENKRONIZASYON KOORDÄ°NATÃ–RÃœ
====================================

TÃœM KOPYALARI SENKRON VE GÃœNCEL TUTAR!

BÄ°R LoRA GÃœNCELLENSE:
1. Ana dosya gÃ¼ncellenir (â­_AKTIF_EN_IYILER)
2. TÃœM kopyalar senkronize edilir
   - Einstein Hall
   - Hybrid Hall
   - Darwin Hall
   - Newton Hall
   - TakÄ±m uzmanlÄ±klarÄ±
   - VS klasÃ¶rleri
3. Population history kaydeder
4. Auditor kontrol eder

AMAÃ‡: HiÃ§bir kopya eski kalmasÄ±n!
"""

import os
import torch
from datetime import datetime
from typing import Dict, List, Set, Tuple
from pathlib import Path


class LoRASyncCoordinator:
    """
    LoRA kopyalarÄ±nÄ± senkronize eder ve denetler
    """
    
    def __init__(self, base_dir: str = "en_iyi_loralar"):
        self.base_dir = base_dir
        
        # Her LoRA'nÄ±n tÃ¼m kopyalarÄ±nÄ±n konumlarÄ± {lora_id: [paths]}
        self.lora_copy_map = {}
        
        # Son senkronizasyon zamanlarÄ± {lora_id: timestamp}
        self.last_sync = {}
        
        # Senkronizasyon sayacÄ±
        self.sync_count = 0
        
        print(f"ðŸ”„ LoRA Sync Coordinator baÅŸlatÄ±ldÄ±")
    
    def register_lora_copy(self, lora_id: str, lora_name: str, file_path: str):
        """
        Bir LoRA kopyasÄ±nÄ± kaydet
        """
        
        if lora_id not in self.lora_copy_map:
            self.lora_copy_map[lora_id] = {
                'name': lora_name,
                'copies': set()
            }
        
        self.lora_copy_map[lora_id]['copies'].add(file_path)
    
    def sync_all_copies(self, lora, match_idx: int, population_history=None, reason: str = "UPDATE"):
        """
        Bir LoRA'nÄ±n TÃœM kopyalarÄ±nÄ± senkronize et!
        
        Args:
            lora: LoRA instance (ana kaynak)
            match_idx: MaÃ§ numarasÄ±
            population_history: History kaydedici (opsiyonel)
            reason: Senkronizasyon sebebi
        
        Returns:
            {
                'synced_count': int,
                'failed_count': int,
                'locations': List[str]
            }
        """
        
        try:
            lora_id = lora.id
            lora_name = lora.name
            
            # Debug: BaÅŸlangÄ±Ã§
            if match_idx % 10 == 0:  # Sadece her 10 maÃ§ta print
                print(f"      ðŸ” DEBUG: Sync baÅŸlatÄ±lÄ±yor â†’ {lora_name[:25]}")
            
            # Bu LoRA'nÄ±n tÃ¼m kopyalarÄ±nÄ± bul
            all_copies = self._find_all_copies(lora_id, lora_name)
            
            if match_idx % 10 == 0:
                print(f"         â€¢ {len(all_copies)} kopya bulundu")
        
        except Exception as e:
            print(f"      âŒ HATA: Sync baÅŸlatÄ±lamadÄ±!")
            print(f"      âŒ LoRA: {lora.name if hasattr(lora, 'name') else 'Unknown'}")
            print(f"      âŒ Hata: {str(e)}")
            return {'synced_count': 0, 'failed_count': 0, 'locations': []}
        
        if len(all_copies) == 0:
            return {
                'synced_count': 0,
                'failed_count': 0,
                'locations': []
            }
        
        # Ana veriyi hazÄ±rla
        main_data = {
            'lora_params': lora.get_all_lora_params(),
            'metadata': {
                'id': lora.id,
                'name': lora.name,
                'generation': lora.generation,
                'birth_match': lora.birth_match,
                'fitness_history': lora.fitness_history,
                'life_energy': getattr(lora, 'life_energy', 1.0),
                'temperament': getattr(lora, 'temperament', {}),
                '_tes_scores': getattr(lora, '_tes_scores', {}),
                '_lazarus_lambda': getattr(lora, '_lazarus_lambda', 0.5),
                '_langevin_temp': getattr(lora, '_langevin_temp', 0.01),
                '_nose_hoover_xi': getattr(lora, '_nose_hoover_xi', 0.0),
                '_kinetic_energy': getattr(lora, '_kinetic_energy', 0.0),
                '_om_action': getattr(lora, '_om_action', 0.0),
                '_ghost_potential': getattr(lora, '_ghost_potential', 0.0),
                'sync_info': {
                    'last_sync_match': match_idx,
                    'last_sync_time': datetime.now().isoformat(),
                    'sync_reason': reason
                }
            }
        }
        
        # TÃ¼m kopyalarÄ± gÃ¼ncelle
        synced_count = 0
        failed_count = 0
        synced_locations = []
        
        try:
            for copy_path in all_copies:
                try:
                    # Mevcut dosyayÄ± yÃ¼kle (metadata'yÄ± korumak iÃ§in)
                    if os.path.exists(copy_path):
                        existing_data = torch.load(copy_path, map_location='cpu')
                        existing_metadata = existing_data.get('metadata', {})
                        
                        # Ã–zel metadata'yÄ± koru (team, specialization_key, score, vs.)
                        preserved_keys = ['team', 'specialization_key', 'score', 'match_count', 'exported_at']
                        for key in preserved_keys:
                            if key in existing_metadata:
                                main_data['metadata'][key] = existing_metadata[key]
                    
                    # GÃ¼ncel veriyi kaydet
                    torch.save(main_data, copy_path)
                    synced_count += 1
                    synced_locations.append(copy_path)
                    
                except Exception as e:
                    failed_count += 1
                    print(f"      âš ï¸  Senkronizasyon hatasÄ±: {copy_path}")
                    print(f"         Hata: {str(e)}")
        
        except Exception as e:
            print(f"      âŒ HATA: Kopyalar gÃ¼ncellenirken hata!")
            print(f"      âŒ Hata: {str(e)}")
            import traceback
            traceback.print_exc()
        
        # Son senkronizasyon zamanÄ±nÄ± gÃ¼ncelle
        self.last_sync[lora_id] = datetime.now()
        self.sync_count += 1
        
        # Population history'ye kaydet (eÄŸer verilmiÅŸse)
        if population_history:
            try:
                population_history.record_lora_event(
                    lora.id,
                    lora.name,
                    match_idx,
                    'SYNC',
                    {
                        'synced_copies': synced_count,
                        'failed_copies': failed_count,
                        'total_copies': len(all_copies),
                        'reason': reason
                    }
                )
            except:
                pass
        
        return {
            'synced_count': synced_count,
            'failed_count': failed_count,
            'locations': synced_locations
        }
    
    def _find_all_copies(self, lora_id: str, lora_name: str) -> Set[str]:
        """
        Bir LoRA'nÄ±n tÃ¼m kopyalarÄ±nÄ± bul
        """
        
        all_copies = set()
        
        # Base directory'de ara
        for root, dirs, files in os.walk(self.base_dir):
            for file in files:
                if file.endswith('.pt'):
                    # Dosya adÄ±nda LoRA ID veya adÄ± geÃ§iyor mu?
                    if lora_id in file or lora_id[:16] in file or lora_id[:8] in file:
                        file_path = os.path.join(root, file)
                        all_copies.add(file_path)
                    elif lora_name in file:
                        file_path = os.path.join(root, file)
                        all_copies.add(file_path)
        
        # Kendi map'imizde varsa ekle
        if lora_id in self.lora_copy_map:
            all_copies.update(self.lora_copy_map[lora_id]['copies'])
        
        return all_copies
    
    def sync_entire_population(self, population: List, match_idx: int, population_history=None) -> Dict:
        """
        TÃœM popÃ¼lasyonu senkronize et!
        
        Her 10 maÃ§ta Ã§aÄŸrÄ±lmalÄ±
        """
        
        print(f"\nðŸ”„ TOPLU SENKRONIZASYON BAÅžLIYOR (MaÃ§ #{match_idx})...")
        
        try:
            print(f"   ðŸ” DEBUG: {len(population)} LoRA senkronize edilecek...")
            
            total_synced = 0
            total_failed = 0
            loras_with_copies = 0
            
            for lora in population:
                result = self.sync_all_copies(lora, match_idx, population_history, reason="PERIODIC_SYNC")
                
                if result['synced_count'] > 0:
                    loras_with_copies += 1
                    total_synced += result['synced_count']
                    total_failed += result['failed_count']
            
            print(f"   âœ… {loras_with_copies} LoRA senkronize edildi")
            print(f"   ðŸ“ Toplam {total_synced} dosya gÃ¼ncellendi")
            
            if total_failed > 0:
                print(f"   âš ï¸  {total_failed} dosya baÅŸarÄ±sÄ±z")
            
            print(f"   ðŸ” DEBUG: Sync tamamlandÄ± baÅŸarÄ±yla!")
            
            return {
                'loras_synced': loras_with_copies,
                'files_synced': total_synced,
                'files_failed': total_failed
            }
            
        except Exception as e:
            print(f"   âŒ HATA: Toplu senkronizasyon baÅŸarÄ±sÄ±z!")
            print(f"   âŒ Hata: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                'loras_synced': 0,
                'files_synced': 0,
                'files_failed': 999
            }
    
    def verify_sync_integrity(self, lora_id: str, lora_name: str) -> Dict:
        """
        Bir LoRA'nÄ±n tÃ¼m kopyalarÄ±nÄ±n tutarlÄ± olduÄŸunu doÄŸrula
        """
        
        all_copies = self._find_all_copies(lora_id, lora_name)
        
        if len(all_copies) == 0:
            return {
                'is_consistent': True,
                'total_copies': 0,
                'issues': []
            }
        
        # Ä°lk dosyayÄ± referans al
        reference_data = None
        reference_path = None
        
        for copy_path in all_copies:
            try:
                data = torch.load(copy_path, map_location='cpu')
                if reference_data is None:
                    reference_data = data
                    reference_path = copy_path
                    break
            except:
                continue
        
        if reference_data is None:
            return {
                'is_consistent': False,
                'total_copies': len(all_copies),
                'issues': ['NO_VALID_REFERENCE']
            }
        
        # DiÄŸer kopyalarÄ± referansla karÅŸÄ±laÅŸtÄ±r
        issues = []
        
        for copy_path in all_copies:
            if copy_path == reference_path:
                continue
            
            try:
                data = torch.load(copy_path, map_location='cpu')
                
                # Parametreleri karÅŸÄ±laÅŸtÄ±r
                ref_params = reference_data.get('lora_params', {})
                copy_params = data.get('lora_params', {})
                
                # Parametre sayÄ±sÄ± aynÄ± mÄ±?
                if len(ref_params) != len(copy_params):
                    issues.append({
                        'type': 'PARAM_COUNT_MISMATCH',
                        'file': copy_path,
                        'expected': len(ref_params),
                        'actual': len(copy_params)
                    })
                    continue
                
                # Her parametre aynÄ± mÄ±?
                for key in ref_params:
                    if key not in copy_params:
                        issues.append({
                            'type': 'MISSING_PARAM',
                            'file': copy_path,
                            'param': key
                        })
                    elif not torch.equal(ref_params[key], copy_params[key]):
                        issues.append({
                            'type': 'PARAM_MISMATCH',
                            'file': copy_path,
                            'param': key
                        })
                
            except Exception as e:
                issues.append({
                    'type': 'LOAD_ERROR',
                    'file': copy_path,
                    'error': str(e)
                })
        
        return {
            'is_consistent': len(issues) == 0,
            'total_copies': len(all_copies),
            'reference': reference_path,
            'issues': issues
        }
    
    def get_sync_stats(self) -> Dict:
        """
        Senkronizasyon istatistikleri
        """
        
        total_loras = len(self.lora_copy_map)
        total_copies = sum(len(data['copies']) for data in self.lora_copy_map.values())
        
        return {
            'total_loras_tracked': total_loras,
            'total_copies_tracked': total_copies,
            'total_syncs_performed': self.sync_count,
            'average_copies_per_lora': total_copies / total_loras if total_loras > 0 else 0
        }


# Global instance
lora_sync_coordinator = LoRASyncCoordinator()

