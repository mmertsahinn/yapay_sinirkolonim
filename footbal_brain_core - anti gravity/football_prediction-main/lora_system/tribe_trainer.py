"""
ğŸ”¥ TRIBE TRAINER - Kabile BazlÄ± Toplu EÄŸitim
============================================

Elek sisteminin (Sieve) bulduÄŸu kabileleri topluca eÄŸitir.
Her kabilenin bir "Lideri" (Chieftain) seÃ§ilir.
Kabile Ã¼yeleri, Lider'in bilgeliÄŸini (Knowledge Distillation) kopyalar.

Bu sayede "aynÄ± hatayÄ± yapanlar", aralarÄ±ndaki "doÄŸruyu yapan" kiÅŸiden ders alÄ±r.
"""

import torch
import numpy as np
from typing import Dict, List, Any

class TribeTrainer:
    """
    Kabile EÄŸitmeni
    """
    
    def __init__(self, distiller, device='cpu'):
        self.distiller = distiller
        self.device = device
        
    def train_tribes(self, tribes: Dict[int, List[Any]], replay_buffer):
        """
        Her kabile iÃ§in toplu eÄŸitim uygula.
        
        1. Kabile Liderini SeÃ§ (En yÃ¼ksek fitness)
        2. Buffer'dan Ã¶rneklem al
        3. TÃ¼m kabile Ã¼yelerini Lider'e benzet (Distillation)
        """
        if not tribes:
            return
            
        print(f"\nğŸ”¥ TRIBE TRAINING: {len(tribes)} kabile kampta...")
        
        # Buffer'dan eÄŸitim verisi al (Son 32 maÃ§ veya Ã¶nemli anlar)
        batch = replay_buffer.sample(batch_size=32)
        if not batch:
            print("   âš ï¸ Buffer boÅŸ, eÄŸitim yapÄ±lamadÄ±.")
            return
            
        # Veriyi hazÄ±rla
        features_np = np.stack([b['features'] for b in batch])
        base_proba_np = np.stack([b['base_proba'] for b in batch])
        # Actual class idx (Hard target iÃ§in gerekirse)
        # actual_indices = torch.tensor([b['actual_class_idx'] for b in batch], device=self.device)
        
        # Her kabile iÃ§in dÃ¶ngÃ¼
        for cluster_id, members in tribes.items():
            if len(members) < 2:
                continue # Tek kiÅŸilik kabilede eÄŸitim olmaz
                
            # 1. Lideri SeÃ§ (Chieftain)
            chieftain = max(members, key=lambda l: l.get_recent_fitness())
            
            # EÄŸer lider bile baÅŸarÄ±sÄ±zsa (fitness < 0.5), dÄ±ÅŸarÄ±dan (global elit) bir mentor ata?
            # Åimdilik sadece kabile iÃ§i.
            
            print(f"   â›º Kabile #{cluster_id} (N={len(members)}): Lider {chieftain.name} ({chieftain.get_recent_fitness():.2f})")
            
            # Liderin Ã§Ä±ktÄ±larÄ±nÄ± al (Soft Targets)
            chieftain.eval() # Gradient yok
            with torch.no_grad():
                # Input hazÄ±rlÄ±ÄŸÄ± (Toplu)
                # LoRA predict methodu tekil Ã§alÄ±ÅŸÄ±yor, burada batch iÅŸlem lazÄ±m.
                # Manuel forward yapalÄ±m.
                x_input = np.concatenate([features_np, base_proba_np], axis=1).astype(np.float32)
                x_tensor = torch.from_numpy(x_input).to(self.device)
                
                teacher_logits = chieftain.forward(x_tensor)
                teacher_probs = torch.softmax(teacher_logits, dim=-1) # Veya zaten prob dÃ¶nÃ¼yorsa direkt
            
            # 2. Ãœyeleri EÄŸit (Intra-Tribe Distillation)
            train_count = 0
            for student in members:
                if student.id == chieftain.id:
                    continue # Lider kendini eÄŸitmez
                    
                # Ã–ÄŸrenciyi eÄŸit
                # Loss = KL(Student, Chieftain)
                # Sadece distillation loss (Hard target yok, Ã§Ã¼nkÃ¼ amaÃ§ lideri taklit etmek)
                
                optimizer = torch.optim.Adam(student.parameters(), lr=0.001)
                student.train()
                
                optimizer.zero_grad()
                student_logits = student.forward(x_tensor)
                # student_log_probs = torch.log_softmax(student_logits, dim=-1) # EÄŸer forward logit dÃ¶nÃ¼yorsa
                
                # VarsayÄ±m: LoRA forward prob dÃ¶nÃ¼yor (softmaxli)
                # O zaman log almalÄ±yÄ±z
                student_log_probs = torch.log(student_logits + 1e-10)
                
                loss = torch.nn.KLDivLoss(reduction='batchmean')(student_log_probs, teacher_probs)
                
                loss.backward()
                optimizer.step()
                train_count += 1
                
            # print(f"      -> {train_count} Ã¼ye eÄŸitildi.")
            
        print("   âœ… Kabile eÄŸitimi tamamlandÄ±.")
