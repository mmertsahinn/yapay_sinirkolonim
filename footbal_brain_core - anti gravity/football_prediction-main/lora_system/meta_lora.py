"""
ğŸ§  META-LoRA - Attention MekanizmasÄ±
====================================

Meta-LoRA: "Hangi LoRA'yÄ± dinleyelim?" kararÄ±nÄ± verir.

Attention mekanizmasÄ±:
- Her maÃ§ iÃ§in her LoRA'ya dinamik aÄŸÄ±rlÄ±k verir
- En uygun uzmanlarÄ± devreye sokar
- Ensemble gibi ama LoRA'lar Ã¼zerinde
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Tuple

from .lora_adapter import LoRAAdapter


class MetaLoRA(nn.Module):
    """
    Meta-LoRA: LoRA popÃ¼lasyonunu yÃ¶neten Ã¼st akÄ±l
    """
    
    def __init__(self, input_dim: int = 63, hidden_dim: int = 64):
        super().__init__()
        
        self.input_dim = input_dim
        
        # Query network: MaÃ§ Ã¶zelliklerinden query vektÃ¶rÃ¼ Ã¼retir
        self.query_net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 16)  # 16 boyutlu query
        )
        
        # Key network: Her LoRA'nÄ±n key vektÃ¶rÃ¼nÃ¼ tutar (Ã¶ÄŸrenilecek)
        # Bu dinamik olduÄŸu iÃ§in forward'da hesaplanacak
        
        self.attention_dim = 16
    
    def get_lora_key(self, lora: LoRAAdapter, device='cpu') -> torch.Tensor:
        """
        Her LoRA'nÄ±n 'key' vektÃ¶rÃ¼nÃ¼ Ã¼ret
        Bu, LoRA'nÄ±n Ã¶zelliklerini/uzmanlÄ±ÄŸÄ±nÄ± temsil eder
        """
        # LoRA'nÄ±n parametrelerinden basit bir Ã¶zellik vektÃ¶rÃ¼ Ã§Ä±kar
        params = lora.get_all_lora_params()
        
        # Her katmandan ortalama deÄŸerleri al
        features = []
        for layer in ['fc1', 'fc2', 'fc3']:
            for matrix in ['lora_A', 'lora_B']:
                param = params[layer][matrix].to(device)
                # Ä°statistikler: mean, std, min, max
                features.extend([
                    param.mean().item(),
                    param.std().item()
                ])
        
        # 12 Ã¶zellik var (3 layer * 2 matrix * 2 stat)
        # 16 boyuta pad et
        while len(features) < self.attention_dim:
            features.append(0.0)
        
        key = torch.tensor(features[:self.attention_dim], dtype=torch.float32, device=device)
        return key
    
    def forward(self, match_features: torch.Tensor, lora_population: List[LoRAAdapter], device='cpu'):
        """
        Attention mekanizmasÄ±yla LoRA'larÄ± aÄŸÄ±rlÄ±klandÄ±r
        
        Args:
            match_features: (batch_size, 61) maÃ§ Ã¶zellikleri
            lora_population: LoRA listesi
        
        Returns:
            attention_weights: (batch_size, num_loras) aÄŸÄ±rlÄ±klar
        """
        # Query: MaÃ§ Ã¶zelliklerinden
        query = self.query_net(match_features)  # (batch_size, 16)
        
        # Keys: Her LoRA'dan
        keys = []
        for lora in lora_population:
            key = self.get_lora_key(lora, device)
            keys.append(key)
        
        keys = torch.stack(keys)  # (num_loras, 16)
        
        # Attention scores: query @ keys^T
        # (batch_size, 16) @ (16, num_loras) = (batch_size, num_loras)
        scores = torch.matmul(query, keys.T)
        
        # Softmax â†’ aÄŸÄ±rlÄ±klar
        attention_weights = F.softmax(scores, dim=-1)
        
        return attention_weights
    
    def aggregate_predictions(
        self,
        match_features: np.ndarray,
        base_proba: np.ndarray,
        lora_population: List[LoRAAdapter],
        device='cpu'
    ) -> Tuple[np.ndarray, Dict]:
        """
        TÃ¼m LoRA'lardan tahmin al ve attention ile birleÅŸtir
        
        Args:
            match_features: (78,) = 60 base + 15 historical + 3 base_proba (zaten birleÅŸik!)
            base_proba: (3,) sadece bilgi iÃ§in
        
        Returns:
            aggregated_proba: (3,) nihai tahmin
            info: DetaylÄ± bilgi
        """
        if len(lora_population) == 0:
            # LoRA yoksa base proba'yÄ± dÃ¶ndÃ¼r
            return base_proba, {'attention_weights': [], 'individual_probas': []}
        
        # Match features tensor (78 boyut)
        x = torch.from_numpy(match_features).unsqueeze(0).float().to(device)
        
        # Attention weights hesapla
        with torch.no_grad():
            attention_weights = self.forward(x, lora_population, device)  # (1, num_loras)
            attention_weights = attention_weights.squeeze(0).cpu().numpy()  # (num_loras,)
        
        # Her LoRA'dan tahmin al
        # LoRA.predict iÃ§inde zaten concat yapÄ±yor: lora_features (75) + base_proba (3) = 78
        individual_probas = []
        lora_features = match_features[:75]  # Ä°lk 75 feature (60 base + 15 historical)
        
        for lora in lora_population:
            lora_proba = lora.predict(lora_features, base_proba, device)
            individual_probas.append(lora_proba)
        
        individual_probas = np.array(individual_probas)  # (num_loras, 3)
        
        # Weighted average
        aggregated_proba = np.sum(individual_probas * attention_weights[:, None], axis=0)
        
        # Normalize (gÃ¼venlik)
        aggregated_proba = aggregated_proba / aggregated_proba.sum()
        
        info = {
            'attention_weights': attention_weights,
            'individual_probas': individual_probas,
            'num_loras': len(lora_population)
        }
        
        return aggregated_proba, info
    
    def get_top_loras(
        self,
        match_features: np.ndarray,
        lora_population: List[LoRAAdapter],
        top_k: int = 5,
        device='cpu'
    ) -> List[Tuple[LoRAAdapter, float]]:
        """
        Bu maÃ§ iÃ§in en yÃ¼ksek attention alan top-K LoRA'larÄ± dÃ¶ndÃ¼r
        """
        if len(lora_population) == 0:
            return []
        
        x = torch.from_numpy(match_features).unsqueeze(0).float().to(device)
        
        with torch.no_grad():
            attention_weights = self.forward(x, lora_population, device)
            attention_weights = attention_weights.squeeze(0).cpu().numpy()
        
        # Top-K indeks
        top_indices = np.argsort(attention_weights)[::-1][:top_k]
        
        top_loras = [(lora_population[i], attention_weights[i]) for i in top_indices]
        
        return top_loras


class SimpleMetaLoRA:
    """
    BasitleÅŸtirilmiÅŸ Meta-LoRA (PyTorch olmadan)
    Fitness bazlÄ± aÄŸÄ±rlÄ±klandÄ±rma
    """
    
    def __init__(self):
        self.name = "SimpleMetaLoRA"
    
    def aggregate_predictions(
        self,
        match_features: np.ndarray,
        base_proba: np.ndarray,
        lora_population: List[LoRAAdapter],
        device='cpu'
    ) -> Tuple[np.ndarray, Dict]:
        """
        Fitness bazlÄ± aÄŸÄ±rlÄ±klandÄ±rma
        
        Args:
            match_features: (78,) = 60 base + 15 historical + 3 base_proba
            base_proba: (3,) sadece bilgi iÃ§in
        """
        if len(lora_population) == 0:
            return base_proba, {'attention_weights': [], 'individual_probas': []}
        
        # Her LoRA'dan tahmin al
        # LoRA.predict iÃ§inde base_proba tekrar concat yapÄ±lacak, o yÃ¼zden sadece lora_features gÃ¶nder
        individual_probas = []
        fitnesses = []
        lora_features = match_features[:75]  # Ä°lk 75 feature (60 base + 15 historical)
        
        for lora in lora_population:
            lora_proba = lora.predict(lora_features, base_proba, device)
            individual_probas.append(lora_proba)
            fitnesses.append(lora.get_recent_fitness())
        
        individual_probas = np.array(individual_probas)  # (num_loras, 3)
        fitnesses = np.array(fitnesses)  # (num_loras,)
        
        # Fitness'i aÄŸÄ±rlÄ±k olarak kullan (softmax)
        fitnesses = np.clip(fitnesses, 0.01, 1.0)  # Negatif olmasÄ±n
        weights = np.exp(fitnesses * 5)  # 5: scaling factor
        weights = weights / weights.sum()
        
        # Weighted average
        aggregated_proba = np.sum(individual_probas * weights[:, None], axis=0)
        
        # Normalize
        aggregated_proba = aggregated_proba / aggregated_proba.sum()
        
        info = {
            'attention_weights': weights,
            'individual_probas': individual_probas,
            'num_loras': len(lora_population)
        }
        
        return aggregated_proba, info
    
    def get_top_loras(
        self,
        match_features: np.ndarray,
        lora_population: List[LoRAAdapter],
        top_k: int = 5,
        device='cpu'
    ) -> List[Tuple[LoRAAdapter, float]]:
        """
        Fitness bazlÄ± top-K
        """
        if len(lora_population) == 0:
            return []
        
        sorted_loras = sorted(lora_population, key=lambda x: x.get_recent_fitness(), reverse=True)
        
        return [(lora, lora.get_recent_fitness()) for lora in sorted_loras[:top_k]]

