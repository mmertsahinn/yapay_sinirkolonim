"""
ğŸŒŠ LANGEVIN DÄ°NAMÄ°KLERÄ° (Stokastik Gradyan Ä°niÅŸi â†’ Fiziksel SDE!)
==================================================================

DETERMÄ°NÄ°ZM YOK! KAOS VAR! MATEMATÄ°K TAM!

Langevin Denklemi:
------------------
dÎ¸ = -âˆ‡U(Î¸) dt + âˆš(2T) dW

Nerede:
  â€¢ Î¸: Parametre (LoRA aÄŸÄ±rlÄ±klarÄ±!)
  â€¢ U(Î¸): Potansiyel enerji (Loss fonksiyonu!)
  â€¢ T: SÄ±caklÄ±k (GÃ¼rÃ¼ltÃ¼ seviyesi!)
  â€¢ dW: Wiener sÃ¼reci (Brownian hareket!)

Bu, LoRA'yÄ± deterministik bir "robot"tan, termal banyoda yÃ¼zen bir
"parÃ§acÄ±k"a dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r!
"""

import torch
import math


class LangevinDynamics:
    """
    Langevin SDE ile LoRA parametre gÃ¼ncellemesi
    """
    
    def __init__(
        self,
        base_temperature: float = 0.01,
        dt: float = 0.01,
        adaptive: bool = True
    ):
        """
        Args:
            base_temperature: Temel sÄ±caklÄ±k (T_base)
            dt: Zaman adÄ±mÄ±
            adaptive: NosÃ©-Hoover termostat kullan mÄ±?
        """
        self.T_base = base_temperature
        self.dt = dt
        self.adaptive = adaptive
        
        # NosÃ©-Hoover iÃ§in
        self.xi = {}  # Her LoRA iÃ§in sÃ¼rtÃ¼nme katsayÄ±sÄ±
        self.momentum = {}  # Her LoRA iÃ§in momentum
        
        print(f"ğŸŒŠ Langevin Dynamics baÅŸlatÄ±ldÄ± (T={base_temperature}, adaptive={adaptive})")
    
    def update_parameters(
        self,
        lora,
        gradients: dict,
        temperature: float = None
    ) -> dict:
        """
        LoRA parametrelerini Langevin SDE ile gÃ¼ncelle!
        
        Args:
            lora: LoRA instance
            gradients: {layer_name: gradient_tensor}
            temperature: Opsiyonel sÄ±caklÄ±k (None ise otomatik!)
        
        Returns:
            {
                'T_eff': Efektif sÄ±caklÄ±k,
                'noise_magnitude': GÃ¼rÃ¼ltÃ¼ bÃ¼yÃ¼klÃ¼ÄŸÃ¼,
                'drift_magnitude': SÃ¼rÃ¼klenme bÃ¼yÃ¼klÃ¼ÄŸÃ¼
            }
        """
        # SÄ±caklÄ±k hesapla
        if temperature is None:
            if self.adaptive:
                T_eff = self._compute_adaptive_temperature(lora, gradients)
            else:
                T_eff = self.T_base
        else:
            T_eff = temperature
        
        # Langevin gÃ¼ncellemesi!
        total_drift = 0.0
        total_noise = 0.0
        
        for layer_name, grad in gradients.items():
            # 1) DRÄ°FT: -âˆ‡U(Î¸) dt
            drift = -grad * self.dt
            
            # 2) DÄ°FÃœZYON: âˆš(2T) dW
            noise_std = math.sqrt(2 * T_eff * self.dt)
            noise = torch.randn_like(grad) * noise_std
            
            # 3) TOPLAM GÃœNCELLEME
            delta = drift + noise
            
            # Parametreyi gÃ¼ncelle (lora.lora_A veya lora_B'ye uygula!)
            # (Bu kÄ±sÄ±m lora.update_params() ile yapÄ±lacak!)
            
            # Ä°statistikler
            total_drift += drift.abs().mean().item()
            total_noise += noise.abs().mean().item()
        
        return {
            'T_eff': T_eff,
            'noise_magnitude': total_noise,
            'drift_magnitude': total_drift,
            'noise_to_drift_ratio': total_noise / (total_drift + 1e-8)
        }
    
    def _compute_adaptive_temperature(self, lora, gradients: dict) -> float:
        """
        NosÃ©-Hoover Termostat ile adaptif sÄ±caklÄ±k!
        
        dÎ¾ = (KE - KE_target) dt
        T_eff = KE / (d/2)
        """
        lora_id = lora.id
        
        # Ä°lk kez mi?
        if lora_id not in self.xi:
            self.xi[lora_id] = 0.0
            self.momentum[lora_id] = {}
        
        # 1) KÄ°NETÄ°K ENERJÄ° HESAPLA
        KE = 0.0
        d = 0  # Toplam parametre sayÄ±sÄ±
        
        for layer_name, grad in gradients.items():
            # Momentum = Gradyan
            if layer_name not in self.momentum[lora_id]:
                self.momentum[lora_id][layer_name] = torch.zeros_like(grad)
            
            p = self.momentum[lora_id][layer_name]
            
            # Momentum gÃ¼ncelle
            xi = self.xi[lora_id]
            p_new = p + (-grad - xi * p) * self.dt
            
            # KE += (1/2) ||p||^2
            KE += 0.5 * (p_new ** 2).sum().item()
            d += p_new.numel()
            
            # Kaydet
            self.momentum[lora_id][layer_name] = p_new
        
        # 2) HEDEF KÄ°NETÄ°K ENERJÄ°
        KE_target = (d / 2.0) * self.T_base
        
        # 3) TERMOSTAT GÃœNCELLEMESÄ°
        # dÎ¾ = (KE - KE_target) dt
        dxi = (KE - KE_target) * self.dt * 0.01  # 0.01: damp factor
        self.xi[lora_id] += dxi
        
        # 4) EFEKTÄ°F SICAKLIK
        T_eff = (2.0 * KE) / (d + 1e-8)
        
        # SÄ±nÄ±rla (Ã§ok aÅŸÄ±rÄ± olmasÄ±n!)
        T_eff = max(0.001, min(T_eff, 0.5))
        
        return T_eff
    
    def compute_gradient_variance_temperature(self, lora, window: int = 10) -> float:
        """
        Gradyan varyansÄ±na gÃ¶re sÄ±caklÄ±k (alternatif yÃ¶ntem!)
        
        T(t) = T_base Ã— (1 + Î± Ã— Var(âˆ‡loss))
        """
        # EÄŸer LoRA'nÄ±n son gradyan geÃ§miÅŸi varsa
        if not hasattr(lora, 'gradient_history') or len(lora.gradient_history) < 2:
            return self.T_base
        
        recent_grads = lora.gradient_history[-window:]
        
        # Varyans hesapla
        grad_variance = torch.var(torch.stack(recent_grads)).item()
        
        # Adaptif sÄ±caklÄ±k
        alpha = 0.5
        T = self.T_base * (1.0 + alpha * grad_variance)
        
        return min(T, 0.5)  # Max 0.5


# Global instance
langevin_dynamics = LangevinDynamics(
    base_temperature=0.01,
    dt=0.01,
    adaptive=True  # NosÃ©-Hoover aktif!
)



