"""
ğŸ§¬ DEEP LEARNING OPTIMIZATION - Knowledge Distillation & Era Jumping
==================================================================

Bu modÃ¼l, LoRA'larÄ±n "insan gibi Ã¶ÄŸrenmesini" ve "Ã§aÄŸ atlamasÄ±nÄ±" saÄŸlar.

Sadece deneyimle (hard label) deÄŸil, usta LoRA'larÄ±n olasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ±nÄ± (soft targets)
kopyalayarak (Knowledge Distillation) Ã§ok daha hÄ±zlÄ± Ã¶ÄŸrenirler.

Teknikler:
1. Knowledge Distillation (Hinton et al., 2015) - Ã‡aÄŸ atlama mekanizmasÄ±
2. Collective Backpropagation - SÃ¼rÃ¼ zekasÄ±yla Ã¶ÄŸrenme
3. Specialization-aware Teacher Selection - UzmanlÄ±k bazlÄ± Ã¶ÄŸretmen seÃ§imi

Bilimsel Temel:
- Hinton et al. (2015): "Distilling the Knowledge in a Neural Network"
- Soft targets: Teacher'Ä±n probability distribution'Ä±
- Temperature scaling: T > 1 â†’ daha yumuÅŸak, daha genel bilgi
- Dark knowledge: Teacher'Ä±n gizli bilgisi (logits'lerde saklÄ±)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Tuple, Optional, Any
from collections import defaultdict


class DeepKnowledgeDistiller:
    """
    Bilgi DamÄ±tma Sistemi:
    GenÃ§ LoRA'lar (Student), Usta LoRA'lardan (Teacher) Ã¶ÄŸrenir.
    
    Formula:
    L_total = Î± Ã— L_soft + (1-Î±) Ã— L_hard
    
    L_soft = TÂ² Ã— KL(softmax(logits_s/T), softmax(logits_t/T))
    L_hard = CrossEntropy(logits_s, labels)
    
    where:
    - logits_s = student logits (softmax Ã¶ncesi)
    - logits_t = teacher logits (softmax Ã¶ncesi)
    - T = temperature (default: 2.0)
    - Î± = soft loss weight (default: 0.7)
    """
    
    def __init__(self, temperature: float = 2.0, alpha: float = 0.7, device='cpu'):
        """
        Args:
            temperature: Softmax sÄ±caklÄ±ÄŸÄ± (YÃ¼ksek = daha yumuÅŸak olasÄ±lÄ±klar, daha fazla bilgi)
            alpha: Teacher loss aÄŸÄ±rlÄ±ÄŸÄ± (1-alpha: GerÃ§ek sonuÃ§ aÄŸÄ±rlÄ±ÄŸÄ±)
            device: Computation device
        """
        self.temperature = temperature
        self.alpha = alpha
        self.device = device
        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')
        self.ce_loss = nn.CrossEntropyLoss()
        
        # Teacher selection cache (performance iÃ§in)
        self.teacher_cache = {}  # {student_id: (teacher_id, fitness)}
        
        print(f"âœ… DeepKnowledgeDistiller initialized (T={temperature}, Î±={alpha})")
    
    def find_best_teacher(self, population: List[Any], current_lora: Any) -> Optional[Any]:
        """
        Bir LoRA iÃ§in en iyi Ã¶ÄŸretmeni bul (Fitness ve benzerlik bazlÄ±)
        
        Strateji:
        1. AynÄ± uzmanlÄ±ktan teacher tercih edilir (daha iyi transfer!)
        2. Fitness > 0.8 olmalÄ±
        3. Kendisi olamaz
        4. Cache kullanÄ±lÄ±r (performance iÃ§in)
        
        Args:
            population: LoRA popÃ¼lasyonu
            current_lora: Ã–ÄŸrenci LoRA
            
        Returns:
            Best teacher LoRA veya None
        """
        # Cache kontrolÃ¼
        if current_lora.id in self.teacher_cache:
            cached_teacher_id, cached_fitness = self.teacher_cache[current_lora.id]
            # Cache'deki teacher hala popÃ¼lasyonda ve fitness yeterli mi?
            for lora in population:
                if lora.id == cached_teacher_id and lora.get_recent_fitness() >= 0.75:
                    return lora
        
        # 1. AynÄ± uzmanlÄ±ktan teacher bul (Ã–NCELÄ°K!)
        current_spec = getattr(current_lora, 'specialization', None)
        same_spec_candidates = []
        
        if current_spec:
            for lora in population:
                if (lora.id != current_lora.id and 
                    getattr(lora, 'specialization', None) == current_spec and
                    lora.get_recent_fitness() > 0.75):
                    same_spec_candidates.append(lora)
        
        if same_spec_candidates:
            # AynÄ± uzmanlÄ±ktan en iyisini seÃ§
            best_teacher = max(same_spec_candidates, key=lambda l: l.get_recent_fitness())
            # Cache'e kaydet
            self.teacher_cache[current_lora.id] = (best_teacher.id, best_teacher.get_recent_fitness())
            return best_teacher
        
        # 2. Genel en iyi teacher (uzmanlÄ±k farklÄ± olsa bile)
        general_candidates = [
            l for l in population 
            if l.id != current_lora.id and l.get_recent_fitness() > 0.80
        ]
        
        if not general_candidates:
            return None
        
        best_teacher = max(general_candidates, key=lambda l: l.get_recent_fitness())
        # Cache'e kaydet
        self.teacher_cache[current_lora.id] = (best_teacher.id, best_teacher.get_recent_fitness())
        return best_teacher
    
    def distill_knowledge(self, 
                         student_lora: nn.Module, 
                         teacher_lora: nn.Module, 
                         features_np: np.ndarray, 
                         base_proba_np: np.ndarray,
                         actual_class_idx: int,
                         optimizer: torch.optim.Optimizer) -> float:
        """
        Bir Ã¶ÄŸrenci LoRA'ya, Ã¶ÄŸretmenin bilgisini aktar.
        
        Process:
        1. Teacher'dan soft targets al (temperature scaling ile)
        2. Student'dan logits al
        3. Distillation loss hesapla (KL divergence)
        4. Hard loss hesapla (CrossEntropy)
        5. Combined loss ile optimize et
        
        Args:
            student_lora: Ã–ÄŸrenci LoRA (gÃ¼ncellenecek)
            teacher_lora: Ã–ÄŸretmen LoRA (sabit, eval mode)
            features_np: Feature array [75] (60 base + 15 historical)
            base_proba_np: Base probability [3]
            actual_class_idx: GerÃ§ek sÄ±nÄ±f indexi
            optimizer: Student'Ä±n optimizer'Ä±
            
        Returns:
            Total loss value (float)
        """
        student_lora.train()
        teacher_lora.eval()  # Teacher sabit
        
        # Data preparation
        x = np.concatenate([features_np, base_proba_np]).astype(np.float32)
        x_tensor = torch.from_numpy(x).unsqueeze(0).to(self.device)
        y_tensor = torch.tensor([actual_class_idx], dtype=torch.long, device=self.device)
        
        optimizer.zero_grad()
        
        # Forward pass: LOGITS al (softmax Ã–NCESÄ°!)
        # âœ… forward_logits() kullan (LoRAAdapter'da olmalÄ±!)
        if hasattr(student_lora, 'forward_logits'):
            student_logits = student_lora.forward_logits(x_tensor)
        else:
            # Fallback: forward() kullan ama logits'e Ã§evir (ters softmax - yaklaÅŸÄ±k)
            student_proba = student_lora.forward(x_tensor)
            # Proba'dan logits'e yaklaÅŸÄ±k dÃ¶nÃ¼ÅŸÃ¼m (numerical stability iÃ§in)
            student_logits = torch.log(student_proba + 1e-10)
        
        # Teacher'dan soft targets (eval mode, no grad)
        with torch.no_grad():
            if hasattr(teacher_lora, 'forward_logits'):
                teacher_logits = teacher_lora.forward_logits(x_tensor)
            else:
                # Fallback
                teacher_proba = teacher_lora.forward(x_tensor)
                teacher_logits = torch.log(teacher_proba + 1e-10)
        
        # 1. Distillation Loss (KL Divergence)
        # Softmax temperature scaling
        T = self.temperature
        
        soft_teacher = F.softmax(teacher_logits / T, dim=-1)
        soft_student = F.log_softmax(student_logits / T, dim=-1)
        
        # KL divergence: KL(P_student || P_teacher)
        distillation_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T ** 2)
        
        # 2. Student Loss (Ground Truth - Hard Labels)
        student_loss = self.ce_loss(student_logits, y_tensor)
        
        # 3. Total Loss
        total_loss = self.alpha * distillation_loss + (1.0 - self.alpha) * student_loss
        
        # Backward pass
        total_loss.backward()
        optimizer.step()
        
        return total_loss.item()
    
    def teach_newborn_lora(self, 
                          newborn_lora: nn.Module,
                          population: List[Any],
                          sample_features: np.ndarray = None,
                          sample_base_proba: np.ndarray = None,
                          device='cpu') -> bool:
        """
        ğŸ“ YENÄ° DOÄAN LoRA'YA MASTER'DAN Ã–ÄRET!
        
        Plan: "Yeni doÄŸan bir LoRA, Master bir LoRA'nÄ±n (Fitness > 0.9) 
        beynini Deep Learning (Distillation Loss) ile kopyalayarak baÅŸlayacak."
        
        Args:
            newborn_lora: Yeni doÄŸan LoRA (henÃ¼z hiÃ§bir ÅŸey Ã¶ÄŸrenmemiÅŸ)
            population: Mevcut popÃ¼lasyon (Master bulmak iÃ§in)
            sample_features: Ã–rnek feature'lar (varsa, yoksa random)
            sample_base_proba: Ã–rnek base proba (varsa, yoksa random)
            device: Device
        
        Returns:
            True if teaching successful, False otherwise
        """
        # 1. Master bul (Fitness > 0.9)
        master_candidates = [
            l for l in population 
            if l.get_recent_fitness() > 0.9 and l.id != newborn_lora.id
        ]
        
        if not master_candidates:
            # Master yoksa, en iyi teacher'Ä± bul (Fitness > 0.8)
            master_candidates = [
                l for l in population 
                if l.get_recent_fitness() > 0.8 and l.id != newborn_lora.id
            ]
        
        if not master_candidates:
            return False  # HiÃ§ teacher yok
        
        # En iyi Master'Ä± seÃ§
        master = max(master_candidates, key=lambda l: l.get_recent_fitness())
        
        # 2. Ã–rnek veri hazÄ±rla (yoksa random)
        if sample_features is None:
            sample_features = np.random.randn(75).astype(np.float32)  # 60 base + 15 historical
        
        if sample_base_proba is None:
            sample_base_proba = np.array([0.33, 0.34, 0.33], dtype=np.float32)  # Uniform
        
        # 3. Distillation yap (birkaÃ§ iterasyon)
        optimizer = torch.optim.Adam(newborn_lora.parameters(), lr=0.001)
        
        # 5 iterasyon yeterli (hÄ±zlÄ± Ã¶ÄŸrenme!)
        for iteration in range(5):
            # Random class (Ã¶ÄŸrenme iÃ§in)
            random_class = np.random.randint(0, 3)
            
            try:
                self.distill_knowledge(
                    newborn_lora,
                    master,
                    sample_features,
                    sample_base_proba,
                    random_class,
                    optimizer
                )
            except Exception as e:
                # Hata varsa devam et
                continue
        
        return True
    
    def clear_cache(self):
        """Teacher cache'i temizle (popÃ¼lasyon deÄŸiÅŸtiÄŸinde)"""
        self.teacher_cache.clear()


class CollectiveDeepLearner:
    """
    Kolektif Derin Ã–ÄŸrenme:
    TÃ¼m popÃ¼lasyonun 'KonsensÃ¼s' hatasÄ±ndan ders Ã§Ä±karmasÄ±.
    
    Concept:
    - EÄŸer sÃ¼rÃ¼ (Ã§oÄŸunluk) yanÄ±ldÄ±ysa, herkes bu hatadan payÄ±na dÃ¼ÅŸeni alÄ±r
    - Global hata bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne gÃ¶re hafif bir 'dÃ¼zeltme' sinyali gÃ¶nderilir
    - Sadece yanlÄ±ÅŸ tahmin yapanlar Ã¶ÄŸrenir (doÄŸru tahmin yapanlar zaten iyi)
    
    Bilimsel Temel:
    - Collective Intelligence
    - Swarm Learning
    - Consensus-based Learning
    """
    
    def __init__(self, device='cpu'):
        self.device = device
        self.ce_loss = nn.CrossEntropyLoss()
        
        print(f"âœ… CollectiveDeepLearner initialized")
    
    def collective_backprop(self, 
                           population: List[Any], 
                           features_np: np.ndarray, 
                           base_proba_np: np.ndarray,
                           actual_class_idx: int,
                           global_error_magnitude: float):
        """
        EÄŸer sÃ¼rÃ¼ (Ã§oÄŸunluk) yanÄ±ldÄ±ysa, herkes bu hatadan payÄ±na dÃ¼ÅŸeni alÄ±r.
        
        Args:
            population: LoRA popÃ¼lasyonu
            features_np: Feature array [75]
            base_proba_np: Base probability [3]
            actual_class_idx: GerÃ§ek sÄ±nÄ±f indexi
            global_error_magnitude: Global hata bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (0-1)
                                   YÃ¼ksek = Ã§oÄŸunluk yanÄ±ldÄ±
        """
        if global_error_magnitude < 0.5:
            return  # Hata kÃ¼Ã§Ã¼kse kolektif Ã¶ÄŸrenmeye gerek yok
        
        x = np.concatenate([features_np, base_proba_np]).astype(np.float32)
        x_tensor = torch.from_numpy(x).unsqueeze(0).to(self.device)
        y_tensor = torch.tensor([actual_class_idx], dtype=torch.long, device=self.device)
        
        # Sadece yanlÄ±ÅŸ tahmin yapanlar Ã¶ÄŸrenir
        wrong_loras = []
        
        for lora in population:
            # Tahmin kontrolÃ¼
            with torch.no_grad():
                if hasattr(lora, 'forward_logits'):
                    logits = lora.forward_logits(x_tensor)
                else:
                    proba = lora.forward(x_tensor)
                    logits = torch.log(proba + 1e-10)
                
                pred_idx = logits.argmax(dim=-1).item()
                
                if pred_idx != actual_class_idx:
                    wrong_loras.append(lora)
        
        if not wrong_loras:
            return  # Hepsi doÄŸru tahmin yaptÄ±
        
        # Her yanlÄ±ÅŸ LoRA'ya hafif dÃ¼zeltme sinyali gÃ¶nder
        for lora in wrong_loras:
            # Ã‡ok kÃ¼Ã§Ã¼k learning rate (kolektif Ã¶ÄŸrenme hafif olmalÄ±)
            lora_params = [p for p in lora.parameters() if p.requires_grad]
            if not lora_params:
                continue
            
            optimizer = torch.optim.SGD(lora_params, lr=0.00001)  # Ã‡ok kÃ¼Ã§Ã¼k!
            optimizer.zero_grad()
            
            # Forward
            if hasattr(lora, 'forward_logits'):
                logits = lora.forward_logits(x_tensor)
            else:
                proba = lora.forward(x_tensor)
                logits = torch.log(proba + 1e-10)
            
            loss = self.ce_loss(logits, y_tensor)
            
            # Loss'u global hata ile scale et (hata ne kadar bÃ¼yÃ¼kse o kadar Ã¶ÄŸren)
            weighted_loss = loss * global_error_magnitude * 0.1  # %10 etki
            
            weighted_loss.backward()
            optimizer.step()


# Global instances
_global_distiller = None
_global_collective_learner = None


def get_deep_knowledge_distiller(temperature: float = 2.0, 
                                 alpha: float = 0.7, 
                                 device='cpu') -> DeepKnowledgeDistiller:
    """Global DeepKnowledgeDistiller instance"""
    global _global_distiller
    if _global_distiller is None:
        _global_distiller = DeepKnowledgeDistiller(
            temperature=temperature,
            alpha=alpha,
            device=device
        )
    return _global_distiller


def get_collective_deep_learner(device='cpu') -> CollectiveDeepLearner:
    """Global CollectiveDeepLearner instance"""
    global _global_collective_learner
    if _global_collective_learner is None:
        _global_collective_learner = CollectiveDeepLearner(device=device)
    return _global_collective_learner

