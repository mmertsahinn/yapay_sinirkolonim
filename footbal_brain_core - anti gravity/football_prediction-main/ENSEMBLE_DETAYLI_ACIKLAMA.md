# ğŸ§  ENSEMBLE MODELÄ°NÄ°N DERÄ°N AÃ‡IKLAMASI

## ğŸ¯ ENSEMBLE NEDÄ°R?

Ensemble = "Bir araya getirme" demek. **4 farklÄ± modelin birleÅŸimi**.

Tek model yerine 4 model kullanÄ±p, hepsinin oyunu birleÅŸtiriyoruz.

---

## ğŸ“Š SÄ°STEMDEKÄ° 4 MODEL

### 1. RandomForest (Rastgele Orman)
```python
RandomForestClassifier(n_estimators=100-500)
```

**Ne Yapar:**
- 100-500 tane **karar aÄŸacÄ±** oluÅŸturur
- Her aÄŸaÃ§ farklÄ± veri alt kÃ¼mesiyle eÄŸitilir
- Her aÄŸaÃ§ bir "oy" verir
- Ã‡oÄŸunluk oyuyla karar verir

**GÃ¼Ã§lÃ¼ YanÄ±:** HÄ±zlÄ±, overfit yapmaz, robust

**Ã–rnek:**
```
AÄŸaÃ§ 1: "Ev galip" (60%)
AÄŸaÃ§ 2: "Beraberlik" (55%)
AÄŸaÃ§ 3: "Ev galip" (70%)
...
AÄŸaÃ§ 100: "Ev galip" (65%)

RandomForest Sonucu: "Ev galip" (62%)
```

---

### 2. XGBoost (Extreme Gradient Boosting)
```python
XGBClassifier(max_depth=3-10, learning_rate=0.01-0.3)
```

**Ne Yapar:**
- AÄŸaÃ§larÄ± **sÄ±rayla** eÄŸitir
- Her aÄŸaÃ§, bir Ã¶ncekinin **hatasÄ±nÄ± dÃ¼zeltmeye** Ã§alÄ±ÅŸÄ±r
- Son aÄŸaÃ§, tÃ¼m Ã¶nceki aÄŸaÃ§larÄ±n toplamÄ±dÄ±r

**GÃ¼Ã§lÃ¼ YanÄ±:** En yÃ¼ksek doÄŸruluk, kompleks iliÅŸkileri yakalar

**Ã–rnek:**
```
AÄŸaÃ§ 1: Tahmin yaptÄ±, %40 hata
AÄŸaÃ§ 2: AÄŸaÃ§ 1'in hatasÄ±nÄ± dÃ¼zelt â†’ %25 hata
AÄŸaÃ§ 3: AÄŸaÃ§ 2'nin hatasÄ±nÄ± dÃ¼zelt â†’ %15 hata
...

XGBoost Sonucu: TÃ¼m aÄŸaÃ§larÄ±n toplamÄ±
```

**Matematiksel:**
```
F(x) = fâ‚(x) + fâ‚‚(x) + fâ‚ƒ(x) + ... + fâ‚™(x)

Her fâ‚™ bir aÄŸaÃ§, her biri Ã¶ncekinin hatasÄ±nÄ± dÃ¼zeltir
```

---

### 3. GradientBoosting
```python
GradientBoostingClassifier(learning_rate=0.01-0.3)
```

**Ne Yapar:**
- XGBoost'a benzer, ama daha **konservatif**
- Daha yavaÅŸ Ã¶ÄŸrenir ama daha **stable**
- Overfitting riski daha az

**GÃ¼Ã§lÃ¼ YanÄ±:** Dengeli, gÃ¼venilir, generalize eder iyi

---

### 4. SVC (Support Vector Classifier)
```python
SVC(kernel='rbf', probability=True)
```

**Ne Yapar:**
- Verileri **yÃ¼ksek boyutlu uzaya** taÅŸÄ±r
- En iyi **ayÄ±rÄ±cÄ± dÃ¼zlem** bulur
- Non-linear iliÅŸkileri yakalar

**GÃ¼Ã§lÃ¼ YanÄ±:** KarmaÅŸÄ±k, non-linear patternleri bulur

**GÃ¶rsel:**
```
     Ev Win
      â—  â—
   â—     â—  |  Beraberlik
  â—  â—      |    â—‹  â—‹
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† SVC'nin bulduÄŸu Ã§izgi
  â—‹  â—‹      |      â—‹
     â—‹  â—‹   |  Deplasman Win
```

---

## ğŸ”¥ ENSEMBLE: SOFT VOTING

```python
ensemble = VotingClassifier(
    estimators=[
        ('RandomForest', rf_model),
        ('XGBoost', xgb_model),
        ('GradientBoosting', gb_model),
        ('SVC', svc_model)
    ],
    voting='soft'  # â† KRITIK!
)
```

### "Soft" vs "Hard" Voting

#### HARD VOTING (KullanmÄ±yoruz):
```
RandomForest:    "Ev galip"
XGBoost:         "Ev galip"
GradientBoosting:"Beraberlik"
SVC:             "Deplasman galip"

SonuÃ§: "Ev galip" (Ã§oÄŸunluk 2/4)
```

#### SOFT VOTING (KullanÄ±yoruz!) â­:
```
RandomForest:    Ev: 60%,  Beraberlik: 25%,  Deplasman: 15%
XGBoost:         Ev: 75%,  Beraberlik: 15%,  Deplasman: 10%
GradientBoosting:Ev: 45%,  Beraberlik: 40%,  Deplasman: 15%
SVC:             Ev: 55%,  Beraberlik: 30%,  Deplasman: 15%

ORTALAMA (Ensemble):
Ev: (60+75+45+55)/4 = 58.75%  â† EN YÃœKSEK!
Beraberlik: (25+15+40+30)/4 = 27.5%
Deplasman: (15+10+15+15)/4 = 13.75%

SonuÃ§: "Ev galip" (58.75%)
```

---

## ğŸ§® MATEMATÄ°KSEL FORMÃœL

### Soft Voting FormÃ¼lÃ¼:

```
P_ensemble(class) = (1/N) Ã— Î£ P_i(class)

P_ensemble: Ensemble olasÄ±lÄ±ÄŸÄ±
N: Model sayÄ±sÄ± (bizde 4)
P_i: i'inci modelin olasÄ±lÄ±ÄŸÄ±
Î£: Toplam
```

### Ã–rnek Hesaplama:

Arsenal vs Chelsea maÃ§Ä± iÃ§in:

```
Model 1 (RF):   P(home_win) = 0.62
Model 2 (XGB):  P(home_win) = 0.71
Model 3 (GB):   P(home_win) = 0.58
Model 4 (SVC):  P(home_win) = 0.65

P_ensemble(home_win) = (0.62 + 0.71 + 0.58 + 0.65) / 4
                     = 2.56 / 4
                     = 0.64 (64%)
```

---

## ğŸ¯ NEDEN ENSEMBLE DAHA Ä°YÄ°?

### 1. Hatalar Birbirini NÃ¶tralize Eder

```
Model 1: Formda Ã§ok iyi, xG'de zayÄ±f
Model 2: xG'de Ã§ok iyi, formda zayÄ±f
Model 3: H2H'da Ã§ok iyi, odds'da zayÄ±f
Model 4: Odds'da Ã§ok iyi, H2H'da zayÄ±f

Ensemble: HEPSÄ°NÄ°N GÃœCÃœNÃœ BÄ°RLEÅTÄ°RÄ°R!
```

### 2. Bias-Variance Dengelenir

```
RandomForest: DÃ¼ÅŸÃ¼k bias, orta variance
XGBoost:      DÃ¼ÅŸÃ¼k bias, dÃ¼ÅŸÃ¼k variance
GradientBoosting: Orta bias, dÃ¼ÅŸÃ¼k variance
SVC:          Orta bias, orta variance

Ensemble: EN DÃœÅÃœK TOPLAM HATA!
```

### 3. Overfitting AzalÄ±r

```
Tek Model: EÄŸitim verisini ezberleyebilir
Ensemble:  4 farklÄ± model â†’ Ezberleme imkansÄ±z!
```

---

## ğŸ“ˆ SÄ°STEMDEKÄ° AKIM

### EÄŸitim:

```
1. VERÄ° YÃœKLEME
   â†“
2. Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ° (70+ Ã¶zellik)
   â†“
3. TRAIN/TEST SPLIT (%80/%20)
   â†“
4. HER MODEL AYRI EÄÄ°TÄ°LÄ°R:
   
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ RandomForestâ”‚ â† RandomizedSearchCV (20 iterasyon)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   XGBoost   â”‚ â† RandomizedSearchCV (20 iterasyon)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚GradientBoostâ”‚ â† RandomizedSearchCV (20 iterasyon)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚     SVC     â”‚ â† Ã–nceki modelden yÃ¼kle veya eÄŸit
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   â†“
5. ENSEMBLE OLUÅTUR:
   
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    VotingClassifier (SOFT)        â”‚
   â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”    â”‚
   â”‚  â”‚ RF â”‚ â”‚XGB â”‚ â”‚ GB â”‚ â”‚SVC â”‚    â”‚
   â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜    â”‚
   â”‚         ORTALAMA AL                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   â†“
6. CROSS-VALIDATION (5-fold)
   â†“
7. MODEL KAYDET (.joblib)
```

### Tahmin:

```
1. YENÄ° VERÄ° GELÄ°R (Arsenal vs Chelsea)
   â†“
2. Ã–ZELLÄ°KLER OLUÅTURULUR (70+ Ã¶zellik)
   â†“
3. HER MODEL TAHMÄ°N YAPAR:
   
   RF:  [Ev: 62%, Ber: 23%, Dep: 15%]
   XGB: [Ev: 71%, Ber: 18%, Dep: 11%]
   GB:  [Ev: 58%, Ber: 27%, Dep: 15%]
   SVC: [Ev: 65%, Ber: 22%, Dep: 13%]
   
   â†“
4. ENSEMBLE ORTALAMAYI ALIR:
   
   [(62+71+58+65)/4, (23+18+27+22)/4, (15+11+15+13)/4]
   = [64%, 22.5%, 13.5%]
   
   â†“
5. EN YÃœKSEK OLASILIK SEÃ‡Ä°LÄ°R:
   
   "Ev galip" (64%)
```

---

## ğŸ” KOD Ä°Ã‡Ä°NDE NELER OLUYOR?

### 1. Model EÄŸitimi:

```python
# Her model iÃ§in hyperparameter tuning
for name, (pipeline, params) in models.items():
    random_search = RandomizedSearchCV(
        pipeline, 
        params, 
        n_iter=20,    # 20 farklÄ± kombinasyon dene
        cv=3,         # 3-fold cross-validation
        n_jobs=-1     # TÃ¼m CPU'larÄ± kullan
    )
    random_search.fit(X_train, y_result_train)
    best_models[name] = random_search.best_estimator_
```

**Ne Yapar:**
- Her model iÃ§in 20 farklÄ± parametre kombinasyonu dener
- Her kombinasyonu 3-fold CV ile test eder
- Toplam: 20 Ã— 3 = 60 eÄŸitim her model iÃ§in
- En iyisini seÃ§er

### 2. Ensemble OluÅŸturma:

```python
ensemble = VotingClassifier(
    estimators=[(name, model) for name, model in best_models.items()],
    voting='soft'  # OlasÄ±lÄ±klarÄ± ortala
)

ensemble.fit(X_train, y_result_train)
```

**Ne Yapar:**
- 4 modeli birleÅŸtirir
- `voting='soft'` â†’ OlasÄ±lÄ±klarÄ± ortalar
- TÃ¼m ensemble'Ä± bir kez daha eÄŸitir

### 3. Tahmin:

```python
# app.py'de
probabilities = ensemble_model.predict_proba(input_data)[0]
# â†’ [0.64, 0.225, 0.135]  (Ev, Ber, Dep)

prediction = ensemble_model.predict(input_data)[0]
# â†’ 0 (en yÃ¼ksek indeks)

result = le.inverse_transform([prediction])[0]
# â†’ "home_win"
```

---

## ğŸ’ª ENSEMBLE'IN GÃœCÃœ

### Tek Model:
```
DoÄŸruluk: ~54-59%
```

### Ensemble:
```
DoÄŸruluk: ~58-62%
```

### Fark:
```
+3-5% daha iyi!

45,000 maÃ§ Ã— 3% = 1,350 maÃ§ daha doÄŸru tahmin!
```

---

## ğŸ“ SONUÃ‡

**Ensemble Modeli:**
- âœ… 4 farklÄ± algoritmanÄ±n gÃ¼cÃ¼nÃ¼ birleÅŸtirir
- âœ… Her modelin zayÄ±f yÃ¶nÃ¼nÃ¼ diÄŸerleri kapatÄ±r
- âœ… Soft voting ile olasÄ±lÄ±klarÄ± ortalar
- âœ… Overfitting'i minimize eder
- âœ… En yÃ¼ksek doÄŸruluÄŸu verir

**Bu yÃ¼zden "beyin" diyoruz!** ğŸ§ 

Her model bir nÃ¶ron gibi, ensemble tÃ¼m beyin! ğŸ”¥





