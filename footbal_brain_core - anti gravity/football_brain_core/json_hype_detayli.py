import json
from datetime import datetime
from pathlib import Path
import time
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
from src.ingestion.alternative_hype_scraper import AlternativeHypeScraper
import argparse
import os
import threading
import signal
import sys

# Global deÄŸiÅŸkenler
scraper_lock = threading.Lock()
global_scraper = None
full_data_global = None  # Son iÅŸlenen veriyi saklamak iÃ§in


def signal_handler(sig, frame):
    """Terminal kapatÄ±lÄ±rken Ã§alÄ±ÅŸan handler"""
    print("\n\nâš ï¸ PROGRAM KAPATILIYOR...", flush=True)

    if full_data_global is not None:
        print("ğŸ’¾ Son veriler diske yazÄ±lÄ±yor...", flush=True)
        try:
            save_to_disk(full_data_global)
            print("âœ… Veriler baÅŸarÄ±yla kaydedildi!", flush=True)
        except Exception as e:
            print(f"âŒ KayÄ±t hatasÄ±: {e}", flush=True)

    print("ğŸ›‘ Program kapatÄ±ldÄ±.", flush=True)
    sys.exit(0)


# Signal handler'Ä± kayÄ±t et
signal.signal(signal.SIGINT, signal_handler)
if sys.platform == "win32":
    signal.signal(signal.SIGTERM, signal_handler)


def get_scraper():
    """Global scraper nesnesini dÃ¶ndÃ¼r (thread-safe)"""
    global global_scraper
    with scraper_lock:
        if global_scraper is None:
            global_scraper = AlternativeHypeScraper()
    return global_scraper

# TakÄ±m listesi
TEAMS = [
    "Galatasaray", "FenerbahÃ§e", "BeÅŸiktaÅŸ", "Trabzonspor", "BaÅŸakÅŸehir FK", "KasÄ±mpaÅŸa", "Ã‡aykur Rizespor", "Sivasspor", "Antalyaspor", "Alanyaspor", "Gaziantep FK", "MKE AnkaragÃ¼cÃ¼", "Hatayspor", "Kayserispor", "Adana Demirspor", "GÃ¶ztepe", "EyÃ¼pspor", "Bodrum FK",
    "Manchester City", "Arsenal", "Liverpool", "Manchester United", "Chelsea", "Tottenham", "Newcastle United", "Aston Villa", "West Ham United", "Brighton", "Bournemouth", "Wolverhampton", "Crystal Palace", "Fulham", "Brentford", "Everton", "Nottingham Forest", "Ipswich Town", "Leicester City", "Southampton",
    "Real Madrid", "FC Barcelona", "AtlÃ©tico Madrid", "Athletic Bilbao", "Real Sociedad", "Villarreal", "Real Betis", "Valencia", "Sevilla", "Girona", "Osasuna", "Rayo Vallecano", "Getafe", "Mallorca", "Las Palmas", "Deportivo AlavÃ©s", "Celta Vigo", "LeganÃ©s", "Valladolid", "Espanyol",
    "Inter", "AC Milan", "Juventus", "Napoli", "Roma", "Lazio", "Atalanta", "Fiorentina", "Bologna", "Torino", "Udinese", "Genoa", "Sampdoria", "Lecce", "Empoli", "Verona", "Monza", "Parma", "Venezia", "Como",
    "Bayern MÃ¼nih", "Borussia Dortmund", "RB Leipzig", "Bayer Leverkusen", "Stuttgart", "Wolfsburg", "MÃ¶nchengladbach", "Eintracht Frankfurt", "Werder Bremen", "Freiburg", "Mainz 05", "Augsburg", "KÃ¶ln", "Bochum", "Hoffenheim", "Union Berlin", "Heidenheim", "St. Pauli",
    "Paris Saint-Germain", "AS Monaco", "Olympique Lyon", "Olympique Marseille", "Lille", "Rennes", "Nice", "Montpellier", "Nantes", "Reims", "Strasbourg", "Lens", "Toulouse", "Lorient", "Auxerre", "Brest", "Metz", "Angers",
    "Benfica", "Porto", "Sporting CP", "Braga", "VitÃ³ria de GuimarÃ£es", "FamalicÃ£o", "Boavista", "Casa Pia", "Portimonense", "Rio Ave", "Estoril", "Moreirense", "Farense", "Gil Vicente", "Estrela da Amadora", "Nacional", "Santa Clara", "AVS Futebol SAD"
]

JSON_FILE = "football_brain_export.json"
MAX_WORKERS = 10


def get_json_path():
    """JSON dosyasÄ±nÄ±n yolunu dÃ¶ndÃ¼rÃ¼r."""
    return Path(__file__).parent / JSON_FILE


def save_to_disk(full_data: Dict):
    """Veriyi diske yazar."""
    path = get_json_path()
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(full_data, f, ensure_ascii=False, indent=2, default=str)


def scrape_match_data(match: Dict):
    """Tek bir maÃ§ iÃ§in hype verisini detaylÄ± ÅŸekilde Ã§eker (olmuyorsa atlar, tekrar denemez)."""
    home = match.get('home_team_name')
    away = match.get('away_team_name')
    league = match.get('league_name')
    date_str = match.get('match_date')

    if not home or not away:
        print(f"âš ï¸ GeÃ§ersiz maÃ§ verisi: {home} vs {away}", flush=True)
        return None

    try:
        print(f"ğŸ” {home} vs {away} iÃ§in hype verisi Ã§ekiliyor (detaylÄ± araÅŸtÄ±rma)...", flush=True)
        scraper = get_scraper()
        match_date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        result = scraper.get_match_hype(league, home, away, match_date)

        if result and result.get("total_mentions", 0) > 0:
            print(f"âœ… {home} vs {away}: {result.get('total_mentions', 0)} tweet bulundu", flush=True)
            return result
        else:
            print(f"âš ï¸ {home} vs {away}: Veri bulunamadÄ±, atlanÄ±yor", flush=True)
            return None

    except Exception as e:
        print(f"âŒ Hata ({home} vs {away}): {e} - AtlanÄ±yor", flush=True)
        return None


# Komut satÄ±rÄ± argÃ¼manlarÄ±nÄ± iÅŸlemek iÃ§in argparse kullanÄ±mÄ±
def parse_arguments():
    parser = argparse.ArgumentParser(description="JSON Hype DetaylÄ± Doldurucu")
    parser.add_argument("--threads", type=int, default=10, help="Thread sayÄ±sÄ±nÄ± belirtin (varsayÄ±lan: 10)")
    parser.add_argument("--log_interval", type=int, default=25, help="YazdÄ±rma sÄ±klÄ±ÄŸÄ±nÄ± maÃ§ sayÄ±sÄ± cinsinden belirtin (varsayÄ±lan: 25 maÃ§)")
    return parser.parse_args()


def main():
    global full_data_global

    args = parse_arguments()
    max_workers = args.threads
    log_interval = args.log_interval

    print("=" * 60)
    print("ğŸš€ JSON HYPE DETAYLI DOLDURUCU (UZUN SÃœRELÄ° MOD)")
    print(f"ğŸ“‚ Hedef Dosya: {get_json_path().name}")
    print(f"ğŸ”§ Thread SayÄ±sÄ±: {max_workers}")
    print(f"ğŸ”„ Retry SayÄ±sÄ±: 5 (detaylÄ± Ã§ekim iÃ§in)")
    print("=" * 60, flush=True)

    while True:
        # JSON dosyasÄ±nÄ± yÃ¼kle
        path = get_json_path()
        try:
            with open(path, 'r', encoding='utf-8') as f:
                full_data = json.load(f)
                full_data_global = full_data  # Global deÄŸiÅŸkene ata
        except Exception as e:
            print(f"âŒ Kritik Hata: Dosya okunamadÄ±! {e}", flush=True)
            time.sleep(5)
            continue

        all_matches = full_data.get('data', {}).get('matches', [])

        if not all_matches:
            print("âŒ Dosyada maÃ§ bulunamadÄ±!", flush=True)
            time.sleep(5)
            continue

        # Belirtilen takÄ±mlarÄ± filtrele
        todo_list = []
        for idx, match in enumerate(all_matches):
            home = match.get('home_team_name')
            away = match.get('away_team_name')

            if home in TEAMS or away in TEAMS:
                is_hype_missing = (match.get('hype_updated_at') is None) or \
                                  (match.get('home_support') is None) or \
                                  (match.get('total_tweets') == 0)

                if is_hype_missing:
                    todo_list.append((idx, match))

        if not todo_list:
            print("ğŸ‰ YapÄ±lacak iÅŸ kalmadÄ±!", flush=True)
            time.sleep(5)
            continue

        total_matches = len(all_matches)
        missing_count = len(todo_list)
        print(f"ğŸ“‹ Toplam {total_matches} maÃ§tan {missing_count} tanesinin hype verisi yok", flush=True)
        print(f"ğŸƒ Ä°ÅŸlenecek maÃ§ sayÄ±sÄ±: {missing_count}/{total_matches}", flush=True)

        # Ä°ÅŸleme baÅŸla
        processed_count = 0
        batch_size = max(max_workers, 5)  # DetaylÄ± Ã§ekim iÃ§in batch size'i azalttÄ±k
        start_time = time.time()
        last_log_time = time.time()

        for i in range(0, len(todo_list), batch_size):
            batch = todo_list[i:i + batch_size]

            # Her batch'ten sonra garbage collection yapÄ±p bellek temizle
            import gc
            gc.collect()

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_idx = {
                    executor.submit(scrape_match_data, item[1]): item[0]
                    for item in batch
                }

                for future in as_completed(future_to_idx):
                    original_idx = future_to_idx[future]
                    result = future.result()

                    if result:
                        target_match = all_matches[original_idx]
                        target_match['home_support'] = result.get("home_support", 0.5)
                        target_match['away_support'] = result.get("away_support", 0.5)
                        target_match['sentiment_score'] = result.get("sentiment_score", 0.0)
                        target_match['total_tweets'] = result.get("total_mentions", 0)
                        target_match['hype_updated_at'] = datetime.now().isoformat()
                        processed_count += 1

                        # Her log_interval maÃ§ta bir geri dÃ¶nÃ¼t ver
                        if processed_count % log_interval == 0:
                            elapsed = time.time() - last_log_time
                            minutes = elapsed / 60
                            remaining = len(todo_list) - processed_count
                            tweets = target_match.get('total_tweets', 0)
                            print(f"âœ… {processed_count}/{len(todo_list)} | â±ï¸ {minutes:.2f}dk | ğŸ“Š Kalan: {remaining} / Toplam maÃ§lar: {total_matches} | ğŸ“¢ {tweets} tweet", flush=True)
                            last_log_time = time.time()

            save_to_disk(full_data)
            time.sleep(1)  # Batch'ler arasÄ±nda daha uzun mola

        total_time = (time.time() - start_time) / 60
        print(f"âœ… {len(todo_list)} maÃ§ tamamlandÄ± | â±ï¸ Toplam: {total_time:.1f}dk", flush=True)
        time.sleep(5)


if __name__ == "__main__":
    main()
